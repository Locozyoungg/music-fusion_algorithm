{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook cell content\n",
    "# Title: Music Fusion AI Analysis\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "from src.feature_extraction import extract_features\n",
    "from src.cvae_model import ConditionalVAE\n",
    "from src.utils import normalize_data\n",
    "from src.config import CHECKPOINT_DIR, LATENT_DIM, GENRE_DIM\n",
    "\n",
    "# Load a trained model\n",
    "model_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "model = ConditionalVAE(latent_dim=LATENT_DIM, genre_dim=GENRE_DIM)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Function to visualize embeddings of different genres\n",
    "def visualize_genre_embeddings(genre_embeddings):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for genre, emb in genre_embeddings.items():\n",
    "        plt.scatter(emb[0], emb[1], label=genre)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Genre Embeddings\")\n",
    "    plt.show()\n",
    "\n",
    "# Sample genre embeddings (replace with real embeddings)\n",
    "genre_embeddings = {\n",
    "    'jazz': [0.1, 0.9],\n",
    "    'classical': [0.8, 0.1],\n",
    "    'hiphop': [0.5, 0.5]\n",
    "}\n",
    "\n",
    "# Visualize embeddings\n",
    "visualize_genre_embeddings(genre_embeddings)\n",
    "\n",
    "# Loading and inspecting audio features\n",
    "file_path = 'data/jazz/example.wav'\n",
    "features = extract_features(file_path)\n",
    "normalized_features = normalize_data(features)\n",
    "\n",
    "print(f\"Extracted Features (normalized): {normalized_features}\")\n",
    "\n",
    "# Generate new music based on genre\n",
    "genre_condition = torch.Tensor([1, 0, 0])  # Example: Condition on 'jazz'\n",
    "with torch.no_grad():\n",
    "    latent_sample = torch.randn(1, LATENT_DIM)\n",
    "    generated_music = model.decode(latent_sample, genre_condition)\n",
    "\n",
    "# Plot generated feature representation (spectrogram example)\n",
    "plt.imshow(generated_music.numpy().reshape(128, -1), aspect='auto', origin='lower')\n",
    "plt.title(\"Generated Music (Feature Representation)\")\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and further analysis\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
